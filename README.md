# ğŸŒ Earthquake ELT Pipeline for Social Impact Analysis

A comprehensive ELT (Extract-Load-Transform) data pipeline built with Apache Airflow for analyzing earthquake data in Mexico, demonstrating the power of modern data engineering for disaster preparedness and policy-making.

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Social & Environmental Impact](#social--environmental-impact)
- [Architecture](#architecture)
- [Technology Stack](#technology-stack)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [ELT Pipeline Explanation](#elt-pipeline-explanation)
- [Dashboard Features](#dashboard-features)
- [Development](#development)
- [Troubleshooting](#troubleshooting)

## ğŸ¯ Project Overview

This project implements a complete ELT (Extract-Load-Transform) pipeline for earthquake data analysis, showcasing:

- **Extract**: Automated data extraction from CSV sources (simulating real-time seismic data feeds)
- **Load**: Raw data loading into PostgreSQL without transformation (preserving data integrity)
- **Transform**: In-database transformations using SQL (creating analytics-ready datasets)
- **Orchestration**: Apache Airflow for workflow management and scheduling
- **Visualization**: Interactive Dash dashboard for insights and analysis

## ğŸŒ± Social & Environmental Impact

### Real-World Problem
Mexico is located in one of the most seismically active regions in the world. Understanding earthquake patterns is crucial for:
- **Public Safety**: Early warning systems and evacuation planning
- **Infrastructure**: Building code enforcement and urban planning
- **Policy Making**: Resource allocation for disaster response
- **Research**: Understanding seismic patterns and predicting future events

### Who Benefits?
- **Civil Protection Agencies**: Data-driven emergency response planning
- **Urban Planners**: Informed decisions about construction zones
- **Policy Makers**: Evidence-based resource allocation
- **Researchers**: Historical data analysis and pattern recognition
- **Citizens**: Access to transparent seismic information

### Why ELT?
1. **Data Preservation**: Raw seismic data must be preserved exactly as received for audit trails and scientific accuracy
2. **Continuous Growth**: New earthquakes occur daily, requiring incremental data loads
3. **Evolving Analysis**: As seismology advances, new transformations can be applied to existing raw data without re-extraction
4. **Performance**: In-database transformations leverage PostgreSQL's power for large-scale aggregations
5. **Flexibility**: Data scientists can create new features from raw data without affecting production pipelines

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CSV Data       â”‚
â”‚  (Sismos.csv)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ EXTRACT
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Data       â”‚
â”‚  (Parquet)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ LOAD (No Transform!)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL     â”‚
â”‚  raw_earthquakesâ”‚ â—„â”€â”€â”€ Immutable raw data
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ TRANSFORM (In-Database SQL)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PostgreSQL     â”‚
â”‚analytics_earth..â”‚ â—„â”€â”€â”€ Cleaned & enriched
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dash Dashboard â”‚
â”‚  (Visualizations)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow
1. **Airflow Scheduler** triggers DAG daily
2. **Extract Task** reads CSV and saves to Parquet
3. **Load Task** inserts raw data into PostgreSQL (no transformation)
4. **Validate Task** ensures data integrity
5. **Transform Tasks** clean and enrich data using SQL
6. **Export Task** creates Parquet files for dashboard
7. **Dashboard** reads from analytics layer only

## ğŸ› ï¸ Technology Stack

- **Orchestration**: Apache Airflow 2.7.3
- **Database**: PostgreSQL 13
- **Data Processing**: Pandas, SQLAlchemy
- **Storage**: Parquet (columnar format)
- **Dashboard**: Dash, Plotly
- **Containerization**: Docker, Docker Compose
- **Language**: Python 3.10

## ğŸš€ Quick Start

### Prerequisites
- Docker and Docker Compose installed
- At least 4GB RAM available
- Git (for cloning)

### Step 1: Clone and Setup
```bash
# Clone repository
git clone <your-repo-url>
cd earthquake-elt-pipeline

# Create data directories
mkdir -p data/raw data/analytics

# Place your Sismos.csv in the data/ folder
cp /path/to/Sismos.csv data/
```

### Step 2: Start Services
```bash
# Build and start all services
docker-compose up -d

# Wait for initialization (2-3 minutes)
docker-compose logs -f airflow-init
```

### Step 3: Access Applications
- **Airflow UI**: http://localhost:8080
  - Username: `admin`
  - Password: `admin`
- **Dashboard**: http://localhost:8050
- **PostgreSQL**: `localhost:5432`
  - User: `dwuser`
  - Password: `dwpassword`
  - Database: `earthquake_dw`

### Step 4: Run the Pipeline
1. Open Airflow UI (http://localhost:8080)
2. Enable the `earthquake_elt_pipeline` DAG
3. Click "Trigger DAG" to run manually
4. Monitor execution in Graph or Tree view
5. Once complete, check the dashboard at http://localhost:8050

### Step 5: Stop Services
```bash
# Stop all services
docker-compose down

# Remove all data (careful!)
docker-compose down -v
```

## ğŸ“ Project Structure

```
earthquake-elt-pipeline/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ earthquake_elt_dag.py      # Main ELT pipeline DAG
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py                      # Dash dashboard application
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Raw Parquet files (partitioned)
â”‚   â”œâ”€â”€ analytics/                  # Transformed Parquet files
â”‚   â””â”€â”€ Sismos.csv                  # Source data (place here)
â”œâ”€â”€ config/
â”‚   â””â”€â”€ init_db.sql                 # Database initialization
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ JUSTIFICATION.md            # Social impact justification
â”‚   â””â”€â”€ SETUP.md                    # Detailed setup guide
â”œâ”€â”€ logs/                           # Airflow logs
â”œâ”€â”€ plugins/                        # Custom Airflow plugins
â”œâ”€â”€ docker-compose.yml              # Multi-container orchestration
â”œâ”€â”€ Dockerfile                      # Custom Airflow image
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ .env                            # Environment variables
â””â”€â”€ README.md                       # This file
```

## ğŸ”„ ELT Pipeline Explanation

### What is ELT?
**ELT** (Extract-Load-Transform) differs from **ETL** (Extract-Transform-Load) in the order of operations:

| Step | ETL | ELT |
|------|-----|-----|
| Extract | âœ… Get data | âœ… Get data |
| Transform | âœ… Clean BEFORE load | âŒ NOT here |
| Load | âœ… Load clean data | âœ… Load RAW data |
| Transform | âŒ Already done | âœ… Transform AFTER load |

### Why ELT for Earthquakes?

1. **Immutable Raw Data**
   - Original seismic readings must never be modified
   - Enables re-analysis with improved algorithms
   - Provides audit trail for scientific research

2. **Performance**
   - PostgreSQL can transform millions of rows faster than Python
   - In-database aggregations leverage indexes and query optimization
   - Parallel processing within the database

3. **Flexibility**
   - New transformations don't require re-extraction
   - Data scientists can experiment without affecting raw data
   - Easy to add new features or fix transformation bugs

4. **Scalability**
   - Raw data can be partitioned by date
   - Incremental loads only process new data
   - Analytics layer can be rebuilt from raw at any time

### Pipeline Stages

#### 1. Extract (Python)
```python
# Read CSV and save to Parquet
df = pd.read_csv('Sismos.csv')
df.to_parquet(f'raw/earthquakes_{batch_id}.parquet')
```

#### 2. Load (Python â†’ PostgreSQL)
```python
# Load raw data WITHOUT transformation
df.to_sql('raw_earthquakes', engine, if_exists='append')
# All columns remain as TEXT - no type conversion!
```

#### 3. Transform (SQL in PostgreSQL)
```sql
-- Clean and transform INSIDE the database
INSERT INTO analytics_earthquakes
SELECT 
    TO_DATE(fecha_utc, 'DD/MM/YYYY') as earthquake_date,
    CAST(magnitud AS NUMERIC) as magnitude,
    -- Feature engineering
    CASE 
        WHEN magnitude >= 6.0 THEN 'Major'
        WHEN magnitude >= 4.0 THEN 'Moderate'
        ELSE 'Minor'
    END as magnitude_category
FROM raw_earthquakes;
```

### Key Features

âœ… **Error Handling**
- 3 automatic retries with exponential backoff
- Validation tasks to ensure data quality
- Comprehensive logging

âœ… **Scaling**
- Parquet format for efficient storage
- Partitioned raw data by batch
- In-database SQL transformations
- Indexes on key columns

âœ… **Scheduling**
- Daily execution (@daily schedule)
- Catchup disabled for production
- Configurable intervals

## ğŸ“Š Dashboard Features

### KPIs
- **Total Earthquakes**: Historical count
- **Average Magnitude**: Central tendency
- **Significant Events**: High-risk earthquakes (â‰¥5.0 magnitude)
- **Maximum Magnitude**: Highest recorded

### Visualizations
1. **Magnitude Distribution**: Bar chart showing earthquake intensity categories
2. **Regional Analysis**: Top 10 most seismically active regions
3. **Temporal Patterns**: Time series of earthquake frequency
4. **Geographic Map**: Interactive map with magnitude/location
5. **Depth Analysis**: Distribution by depth category

### Filters
- Magnitude range slider
- Region multi-select dropdown
- Auto-refresh every minute

### Insights
- Most active regions
- Depth patterns
- Risk assessment recommendations

## ğŸ”§ Development

### Adding New Transformations

1. Edit `dags/earthquake_elt_dag.py`
2. Add new SQL in `TRANSFORM_SQL` constant
3. Raw data remains unchanged
4. Test with `docker-compose restart airflow-scheduler`

### Customizing Dashboard

1. Edit `dashboard/app.py`
2. Add new queries to fetch different aggregations
3. Create new Plotly charts
4. Restart: `docker-compose restart dashboard`

### Database Access

```bash
# Connect to PostgreSQL
docker-compose exec postgres psql -U dwuser -d earthquake_dw

# View raw data
SELECT * FROM raw_earthquakes LIMIT 10;

# View transformed data
SELECT * FROM analytics_earthquakes LIMIT 10;

# Check statistics
SELECT * FROM earthquake_statistics;
```

## ğŸ› Troubleshooting

### Airflow won't start
```bash
# Check logs
docker-compose logs airflow-webserver
docker-compose logs airflow-scheduler

# Restart services
docker-compose restart
```

### Dashboard shows no data
```bash
# Verify pipeline ran successfully
# Check Airflow UI for task status

# Verify data in database
docker-compose exec postgres psql -U dwuser -d earthquake_dw -c "SELECT COUNT(*) FROM analytics_earthquakes;"
```

### Database connection errors
```bash
# Check PostgreSQL is running
docker-compose ps postgres

# Test connection
docker-compose exec postgres pg_isready -U dwuser
```

### Performance issues
```bash
# Check resource usage
docker stats

# Increase Docker memory allocation
# Edit Docker Desktop settings â†’ Resources â†’ Memory
```

## ğŸ“š Additional Resources

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [Dash Documentation](https://dash.plotly.com/)
- [Parquet Format](https://parquet.apache.org/)

## ğŸ‘¥ Contributors

- Data Engineering Team
- Social Impact Analysis Group

## ğŸ“„ License

This project is for educational purposes.

---

**Built with â¤ï¸ for disaster preparedness and social impact**